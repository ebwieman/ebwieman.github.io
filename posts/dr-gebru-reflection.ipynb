{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d08a46b6-ea1a-4c7d-b057-9e5d328c666d",
   "metadata": {},
   "source": [
    "---\n",
    "title: Reflecting on Dr. Gebru's Talk\n",
    "author: Eliza Wieman\n",
    "date: '2023-04-16'\n",
    "description: \"This blog post is about Dr. Timnit Gebru, and specifically the talk she gave to our class at Middlebury. The first part contains background information and a question in preparation for her visit to Middlebury, and the second part contains a reflection of the talk she gave while here.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d89a5-09b8-43a7-bdbf-7adae55b128c",
   "metadata": {},
   "source": [
    "## Learning from Dr. Timnit Gebru\n",
    "### Part 1: Background Research and Questions\n",
    "#### General Background\n",
    "Dr. Timnit Gebru is a computer scientist whose work has focused on AI ethics and algorithmic bias. She is a founder of both Black in AI, a group of Black artificial intelligence researchers, and the Distributed Artificial Intelligence Research Institute (DAIR), where she conducts independent research how artificial intelligence impacts marginalized groups, focusing specifically on Africa and African immigrants in the US. Dr. Gebru received her BS, MS and PhD (focused on computer vision) from Stanford University. During her time at Stanford she also worked at both Apple and Microsoft and she took a job focused on ethics in artificial intelligence at Google after completing her PhD. Dr. Gebru is a well-known for her repeated calls to more closely examine the implications of artificial intelligence algorithms and how they are being employed. She was part of a group that urged Amazon to stop selling their race and gender-biased facial recognition software to law enforcement agencies and she co-authored a paper on the risks of large language models, which ultimately led to her firing from Google. Dr. Gebru is also well-known for her work on the Gender Shades project, along with Buolamwini, which evaluates a number of facial recognition softwares on their ability to correctly identify women, finding particularly low accuracy rates for Black women. Since her split from Google in 2020, Dr. Gebru has continued to work on research surrounding the ethical implications of AI, as well as highlighting the institutional racism within companies such as Google and the fields of artificial intelligence and computer vision themselves, which allows many concerns of algorithmic bias to go unnoticed.\n",
    "#### FATE in Computer Vision Talk\n",
    "In 2020, Dr. Gebru gave a 50-minute talk during the Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the Conference on Computer Vision and Pattern Recognition. During this talk, Dr. Gebru discusses a number of problematic applications of artificial intelligence, many of which are related to facial recognition software. She discusses the problematic implications of using these tools for surveillance, which has uneven negative impacts on marginalized groups, but she also discusses other problematic applications. One example she presents is Faceception, which claims it can extract information about someone's mood or personality traits solely based on an image of their face. Dr. Gebru discusses how tools such as this are particularly harmful to marginalized groups because they are built on, and then perpetuate existing societal biases. For example, if certain races are more likely to be viewed as potential terrorists in the US, a tool that predicts personality traits is more likely to classify that race as dangerous. Dr. Gebru discusses how tools such as these are utilized in many automatic hiring processes, disadvantaging already marginalized groups and making it more difficult for them to secure a job.\n",
    "\n",
    "Dr. Gebru discusses the importance of training artificial intelligence tools using diverse datasets to avoid issues such as those highlighted in her Gender Shades work. However, another main point of hers is that simply creating and training models on diverse datasets does not address all of the ethical issues with using artificial intelligence tools. One example she uses to support this argument is the concept of gender prediction algorithms. Even if they use diverse datasets, the idea of gender prediction tools is problematic to begin with because they general enforce a gender binary, often misidentify trans people, and can be used to reinforce existing gender stereotypes (for example, when used for gender-based ad prediction). Dr. Gebru argues that while diverse datasets are definitely necessary, we need to go beyond just thinking about dataset diversity to consider the fundamental purpose of an algorithm, how it is being employed, and who it will harm/benefit. As Dr. Gebru discusses in her talk, many of the issues surrounding computer vision stem from a lack of regulation of tools used within the field, as well as the notion of \"abstraction\", which allows computer scientists to view their research as removed from the real world where much of this research is actively being employed.\n",
    "#### tl;dr\n",
    "Everyone needs to understand that the regulatory policy surrounding computer vision is lagging dramatically behind the technological advancement, and this is leading to infringement upon basic human rights, specifically for already marginalized groups.\n",
    "#### Question for Dr. Gebru\n",
    "How does your current position at an independent research institution, rather than at a large tech conglomerate such as Google, allow you to tackle questions of accountability and ethics in artificial intelligence that you were unable to thoroughly address in your prior work? What is the role of independent institutions such as yours as well as larger companies such as Google and Apple in addressing the unethical employment of these algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06c0ff-8083-4462-b4cf-7dcc31ed60f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
