{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4d89a5-09b8-43a7-bdbf-7adae55b128c",
   "metadata": {},
   "source": [
    "## Learning from Dr. Timnit Gebru\n",
    "### Part 1: Background Research and Questions\n",
    "#### General Background\n",
    "Dr. Timnit Gebru is a computer scientist whose work has focused on AI ethics and algorithmic bias. She is a founder of both Black in AI, a group of Black artificial intelligence researchers, and the Distributed Artificial Intelligence Research Institute (DAIR), where she conducts independent research how artificial intelligence impacts marginalized groups, focusing specifically on Africa and African immigrants in the US. Dr. Gebru received her BS, MS and PhD (focused on computer vision) from Stanford University. During her time at Stanford she also worked at both Apple and Microsoft and she took a job focused on ethics in artificial intelligence at Google after completing her PhD. Dr. Gebru is a well-known for her repeated calls to more closely examine the implications of artificial intelligence algorithms and how they are being employed. She was part of a group that urged Amazon to stop selling their race and gender-biased facial recognition software to law enforcement agencies and she co-authored a paper on the risks of large language models, which ultimately led to her firing from Google. Dr. Gebru is also well-known for her work on the Gender Shades project, along with Buolamwini, which evaluates a number of facial recognition softwares on their ability to correctly identify women, finding particularly low accuracy rates for Black women. Since her split from Google in 2020, Dr. Gebru has continued to work on research surrounding the ethical implications of AI, as well as highlighting the institutional racism within companies such as Google and the fields of artificial intelligence and computer vision themselves, which allows many concerns of algorithmic bias to go unnoticed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad11613-65e5-4db6-aac4-4f25fcdb782a",
   "metadata": {},
   "source": [
    "#### FATE in Computer Vision Talk\n",
    "In 2020, Dr. Gebru gave a 50-minute talk during the Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the Conference on Computer Vision and Pattern Recognition. During this talk, Dr. Gebru discusses a number of problematic applications of artificial intelligence, many of which are related to facial recognition software. She discusses the problematic implications of using these tools for surveillance, which has uneven negative impacts on marginalized groups, but she also discusses other problematic applications. One example she presents is Faceception, which claims it can extract information about someone's mood or personality traits solely based on an image of their face. Dr. Gebru discusses how tools such as this are particularly harmful to marginalized groups because they are built on, and then perpetuate existing societal biases. For example, if certain races are more likely to be viewed as potential terrorists in the US, a tool that predicts personality traits is more likely to classify that race as dangerous. Dr. Gebru discusses how tools such as these are utilized in many automatic hiring processes, disadvantaging already marginalized groups and making it more difficult for them to secure a job.\n",
    "\n",
    "Dr. Gebru discusses the importance of training artificial intelligence tools using diverse datasets to avoid issues such as those highlighted in her Gender Shades work. However, another main point of hers is that simply creating and training models on diverse datasets does not address all of the ethical issues with using artificial intelligence tools. One example she uses to support this argument is the concept of gender prediction algorithms. Even if they use diverse datasets, the idea of gender prediction tools is problematic to begin with because they general enforce a gender binary, often misidentify trans people, and can be used to reinforce existing gender stereotypes (for example, when used for gender-based ad prediction). Dr. Gebru argues that while diverse datasets are definitely necessary, we need to go beyond just thinking about dataset diversity to consider the fundamental purpose of an algorithm, how it is being employed, and who it will harm/benefit. As Dr. Gebru discusses in her talk, many of the issues surrounding computer vision stem from a lack of regulation of tools used within the field, as well as the notion of \"abstraction\", which allows computer scientists to view their research as removed from the real world where much of this research is actively being employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cb6e0-f146-465f-ad40-d9488a05eac8",
   "metadata": {},
   "source": [
    "#### tl;dr\n",
    "Everyone needs to understand that the regulatory policy surrounding computer vision is lagging dramatically behind the technological advancement, and this is leading to infringement upon basic human rights, specifically for already marginalized groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336efd3-34bd-4c51-842b-0f0fa87c3af0",
   "metadata": {},
   "source": [
    "#### Question for Dr. Gebru\n",
    "How does your current position at an independent research institution, rather than at a large tech conglomerate such as Google, allow you to tackle questions of accountability and ethics in artificial intelligence that you were unable to thoroughly address in your prior work? What is the role of independent institutions such as yours as well as larger companies such as Google and Apple in addressing the unethical employment of these algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4181b60-a60c-4506-978d-3db92dcc2097",
   "metadata": {},
   "source": [
    "## Part 2: Reflecting on Dr. Gebru's Talk\n",
    "This Monday, Dr. Gebru gave a talk on \"Eugenics and the Promise of Utopia through Artificial General Intelligence\". Her talk centered around the question of \"utopia for who?\", pointing out that while many proponents of AGI, specifically billionaires funding these ventures, claim that AGI will solve all of our problems in the near future, it is unlear what \"us\" they are addressing. While current rhetoric around AGI suggests that theses technologies will help humanity reach \"unlimited intelligence\" or solve important timely issues such as climate change, the current employment of these algorithms often exacerbates existing inequalities and traumatizes already marginalized groups. Dr. Gebru gave many examples throughout her talk of how this so-called utopia is inaccessible or sometimes actively harmful to groups. One example of this harm is that AGI platforms such as ChatGPT outsource content moderation to countries such as Kenya with very low wage limits ($2/hr or less). Dr. Gebru spoke of one Kenyan worker who was compensated horribly and now suffers from PTSD from moderating horrific content for hours and days on end. Other examples include the large amount of data theft, specifically from artists, that goes into training many of these models. Dr. Gebru also discussed how these generalized models that aim to do \"everything for everyone\" often fall short of that claim, highlighting how numerous large scale translation and language processing models have very limited capabilities for translating African languages.\n",
    "\n",
    "Rather than just discussing how these claims of utopia through AGI are false, Dr. Gebru went on to claim that the desire for an AGI-centered utopia is connected to the eugenics movement. Dr. Gebru contextualized this through first describing first-wave eugenics, the idea that we should breed out humans with undesirbale traits, and then discussing second-wave eugenics, which moves past the notion of \"breeding out\", instead relying on biological interventions to further the human race. The eugenics movement relies strongly on the idea of furthering human intelligence through these mechanisms, using racist metrics such as IQ tests to determine who superior human beings are. Dr. Gebru continued on to explain the TESCREAL bundle, a series of ideologies that she agrues build on past eugenics movements are heavily influencing the current trajectory of AI, and AGI specifically. Dr. Gebru's main argument seemed to be that the rhetoric used to promote AGI is rooted in the eugenics movement and many of the philosophies comprised in the TESCREAL bundle and thus the promise of utopia through AGI is inherently racist and inaccessible to large groups of people. Not only is the promise of utopia through AGI inaccessible, but it is actively harmful, specifically to already marginalized groups.\n",
    "\n",
    "I agree with Dr. Gebru's general agrument that AGI will not solve all of the world's problems, as many of its proponents seem to think, and that it is generally beneficial for certain groups over others. I generally followed the thread of her argument connecting eugenics and AGI, but I thought that the direct link between the two could have been articulated more clearly. I am skeptical about the merits and uses of AGI, but I am also skeptical about some of Dr. Gebru's stronger claims that it is directly linked to eugenics, which by extension suggests that all AGI methods are malicious in intent. I agree with Dr. Gebru's conclusions that smaller models designed to address specific questions are more useful and have the potential to benefit more groups, specifically marginalized ones, and I support this type of work. However, I also think that more powerful or generalized models can be useful, and I do not think that they are all inherently problematic. What I find more problematic, which Dr. Gebru also touched on, is that these large generalized models help concentrate power among people and groups who are not likely to think critically about, or care, who their models are harming versus helping. Overall, I generally agreed with Dr. Gebru's points, but I thought they could have been made more clearly and I wish she had better articulated her argument about how AGI is rooted in eugenics. I do think the talk made me think more critically overall about who creates and promotes the tools I use in my daily life, and how or if this changes how I should use them.\n",
    "\n",
    "## Part 3: Reflecting on the Process\n",
    "I am really glad that we got to engage intentionally with Dr. Gebru's work and meet her. The discussion of ethics in my computer science courses before this one has felt pretty hand-wavy, so I appreciated that this felt like a really in-depth and intentional discussion of ethics and their relationship to our course content. It was also really exciting to get to talk to her directly and ask her a question in class, as she is pretty famous and has done very cool stuff and I never thought I would get a chance to speak to her directly. I was a little bit frustrated by her talk because it seemed a bit disorganized and I felt like I didn't get as much out of it as I was hoping to. But overall, it was really exciting to hear her speak because she is clearly extremely intelligent and passionate about these issues, which was fun to engage with. I also think it is exciting to see someone engaging with ethics in AI as a career path, specifically engaging with ethics and still doing scholarly research that seems really interesting and challenging, because that is not a career path that I am often exposed to. I also found her comments about imposter syndrome and unqualified men in the field very refreshing, because I have definitely been in situations where I have felt more compentent or qualified that my male peers and that has not been reflected in how I have been treated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13912f34-c1f7-4d13-8e5b-1332a5dde235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
