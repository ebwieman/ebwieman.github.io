[
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementing-perceptron",
    "href": "posts/perceptron/perceptron_blog.html#implementing-perceptron",
    "title": "Perceptron Blog",
    "section": "Implementing Perceptron",
    "text": "Implementing Perceptron\nThe perceptron update was implemented using: \\[\\mathbf{\\tilde{w}}^{(t+1)}=\\mathbf{\\tilde{w}}^{(t)}+\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)y_i\\mathbf{\\tilde{x}}_i\\] (Equation 1 in https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-perceptron.html#source-code). This update works by calculating the dot product of the current weights and the features of the selected point and then multiplying it by the true label of that point. If the signs of the dot product and the label are different then the weights are updated, otherwise the weights remain the same. In the case that an update does occur, the new weights are calculated by multiplying the features of the selected point by the actual label and adding that to the current weights. Since we multiply the selected points features’ by the point’s true label, we are always moving our weights in the direction of that label, improving the perceptron’s ability to classify that specific point. By iterating through the points randomly and performing updates, the perceptron algorithm eventually converges if the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#testing-the-algorithm",
    "href": "posts/perceptron/perceptron_blog.html#testing-the-algorithm",
    "title": "Perceptron Blog",
    "section": "Testing the Algorithm",
    "text": "Testing the Algorithm\nThree experiments were performed to verify that the perceptron algorithm was implemented correctly.\n\nExperiment 1: Linearly Separable Data\nFirst the perceptron was fit on linearly separable data with two features.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n100 data points were generated, each with two features and a label of 1 or 0. The data was designed to be linearly separable (meaning the perceptron should converge if run for long enough).\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe algorithm was fit to the data with a maximum of 1000 iterations.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nIf we print the weight vector, we see three values. The first value is the weight of feature 1, the second is the weight of feature 2, and the third is the bias.\n\np.w\n\narray([2.10557404, 3.1165449 , 0.25079936])\n\n\nWe can also plot the accuracy as the model is fit.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIf we print the accuracy, we see that it is 1.0, we classified all of our data correctly!\n\np.score(X, y)\n\n1.0\n\n\nWe see this too when we draw the line specified by our weights. The line perfectly separates the two types of data.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-4,4])\n\n(-4.0, 4.0)\n\n\n\n\n\n\n\nExperiment 2: Not Linearly Separable Data\nNext let’s see how our algorithm runs on data that isn’t linearly separable. First we’ll define some new data. We still have 100 points, each with two features and a label, but when we visualize the data below we can see that it is not clustered as nicely.\n\nnp.random.seed(123)\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nA new perceptron was fit and the weights were printed. Like with Experiment 1, we see a weight vector with three values.\n\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\np2.w\n\narray([ 3.25206473,  1.64360407, -1.10792451])\n\n\nWhen we plot and print the accuracy over time, we see that we were able to achieve an accuracy of 0.89 after 1000 runs. This makes sense, as an accuracy of 1.0 is required for the algorithm to end early, which is impossible on this dataset.\n\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X,y)\n\n0.89\n\n\nWhen we visualize the line that the perceptron specified, we see that it’s pretty good. It seems unlikely that the accuracy could get much better on this dataset.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3.5,3.5])\n\n(-3.5, 3.5)\n\n\n\n\n\n\n\nExperiment 3: Data with More than 2 Features\nFor the last experiment we see if the perceptron can classify data with more than two features. I generated a dataset of 100 points, still with binary labels, but this time each data point has five features.\n\nX, y = make_blobs(n_samples = 100, n_features = 5, centers = 2)\n\nNow when we fit our perceptron and print the weights we get a six element vector, where the first five values correspond to the features and the final value corresponds to the bias. Our perceptron generalizes to data with more than two features!\n\np3 = Perceptron()\np3.fit(X,y,max_steps = 1000)\np3.w\n\narray([ 2.02291666,  6.08425872,  0.74441411,  5.17387787, 10.87406143,\n       -1.69055338])\n\n\nWhen we plot and print the accuracy of our perceptron we see that we achieve a loss of 1.0. This suggests that the dataset I generated is linearly separable.\n\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X,y)\n\n1.0"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#runtime-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#runtime-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Runtime of the Perceptron Update",
    "text": "Runtime of the Perceptron Update\nEach perceptron update (see Equation 1) is performed on only one data point, so the runtime of a single update is independent of the number of data points in the provided dataset. Performing the dot product \\(\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle\\) requires p+1 multiplications and p additions, for a runtime of O(p) (where p = number of features). Multiplying by a scalar increases the runtime of \\(\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)\\) to \\(O(p^2)\\). We then multiply that whole term by \\(y_i\\mathbf{\\tilde{x}}_i\\), a scalar and a vector, for a runtime of \\(O(p^4)\\). Adding the update term to the previous weight takes O(p+1), for a total runtime of \\(O(p^4+p+1)\\), which simplifies to \\(O(p^4)\\)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post implementing the perceptron algorithm and testing it on three datasets ranging in complexity and difficulty to classify.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "testing quarto"
  }
]