[
  {
    "objectID": "posts/bias-audit.html",
    "href": "posts/bias-audit.html",
    "title": "Auditing Racial Bias in an Employment Prediction Tool",
    "section": "",
    "text": "In this blog post, I train a logisitc regression model to predict employment status using a variety of demographic features excluding race, and then I perform a fairness audit to assess whether the model displays racial bias. All data I use is from the American Community Survey. My blog post contains some initial basic exploration of the data, followed by model training, a fairness audit and some concluding discussion.\n\n\nFor this blog post, I chose to examine racial bias in an employment status prediction model. I looked specifically at data from the state of Delaware, as this is where I am from. I trained my model on the list of features suggested in the blog post assignment, which include sex, age, level of schooling, disability status, and nativity, among other features. All features used are explained in detail in the appendix of Ding et al. After choosing my problem, I downloaded the data extracted relevant features and cleaned it up to prepare for training, and then performed a train/test split.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\n# download data\nSTATE = \"DE\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\n# extract relevant features\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      38\n      14.0\n      5\n      16\n      2\n      NaN\n      1\n      1.0\n      4.0\n      8\n      1\n      2\n      2\n      2.0\n      1\n      2\n      6.0\n    \n    \n      1\n      87\n      19.0\n      2\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      8\n      1\n      2\n      2\n      1.0\n      1\n      1\n      6.0\n    \n    \n      2\n      18\n      18.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      8\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      3\n      20\n      19.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      2\n      2\n      6.0\n    \n    \n      4\n      83\n      13.0\n      2\n      16\n      1\n      NaN\n      1\n      1.0\n      2.0\n      8\n      1\n      2\n      2\n      2.0\n      1\n      1\n      6.0\n    \n  \n\n\n\n\n\n# remove race and employment status from the training features\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\n# construct basic problem\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\n\n\nNow that we’ve defined a problem, let’s do some basic descriptive analysis. To do this easily, we’ll recombine our group and label data with the feature data and print them as a single dataframe.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      group\n      label\n    \n  \n  \n    \n      0\n      56.0\n      16.0\n      1.0\n      6.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      8.0\n      1.0\n      2.0\n      1.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      1\n      48.0\n      21.0\n      1.0\n      0.0\n      2.0\n      0.0\n      2.0\n      1.0\n      4.0\n      8.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      2\n      60.0\n      19.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      8.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      3\n      16.0\n      12.0\n      5.0\n      2.0\n      1.0\n      1.0\n      1.0\n      1.0\n      0.0\n      8.0\n      1.0\n      2.0\n      2.0\n      1.0\n      2.0\n      1\n      True\n    \n    \n      4\n      56.0\n      16.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      8.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7293\n      42.0\n      21.0\n      4.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      8.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      7294\n      39.0\n      16.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      7295\n      52.0\n      22.0\n      2.0\n      13.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2\n      True\n    \n    \n      7296\n      46.0\n      21.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      8.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      True\n    \n    \n      7297\n      65.0\n      16.0\n      4.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1\n      False\n    \n  \n\n7298 rows × 17 columns\n\n\n\nOur dataframe has 7298 rows, corresponding to 7298 individuals. If we take the mean of the label column we see that 0.46 of the individuals in the dataset have a label of 1, meaning they are employed.\n\ndf[\"label\"].mean()\n\n0.45765963277610305\n\n\nNext, we’ll look at the racial distribution of the dataset. We see that there are 5535 White individuals, 1168 Black/African American individuals, and 595 individuals of other self-identified racial groups. The largest racial group in the dataset by far is White individuals.\n\ndf.groupby(\"group\").size()\n\ngroup\n1    5535\n2    1168\n3      26\n5      10\n6     267\n7       2\n8     120\n9     170\ndtype: int64\n\n\nIf we look at employment rate by race, we see an employment rate of 0.46 for White individuals and an employment rate of 0.44 for Black/African American individuals.\n\ndf.groupby(\"group\")[[\"label\"]].mean()\n\n\n\n\n\n  \n    \n      \n      label\n    \n    \n      group\n      \n    \n  \n  \n    \n      1\n      0.462511\n    \n    \n      2\n      0.443493\n    \n    \n      3\n      0.461538\n    \n    \n      5\n      0.300000\n    \n    \n      6\n      0.494382\n    \n    \n      7\n      1.000000\n    \n    \n      8\n      0.433333\n    \n    \n      9\n      0.358824\n    \n  \n\n\n\n\nWe can break this down further to look at differences in employment rate by both race and sex. We see higher employment rates for White males, compared with White females. Conversely, we see that Black females have higher employment rates than Black males. The gap between male and female employment rates is 5-6% for both racial groups.\n\ndf.groupby([\"group\",\"SEX\"])[[\"label\"]].mean()\n\n\n\n\n\n  \n    \n      \n      \n      label\n    \n    \n      group\n      SEX\n      \n    \n  \n  \n    \n      1\n      1.0\n      0.490064\n    \n    \n      2.0\n      0.436890\n    \n    \n      2\n      1.0\n      0.411654\n    \n    \n      2.0\n      0.470126\n    \n    \n      3\n      1.0\n      0.428571\n    \n    \n      2.0\n      0.500000\n    \n    \n      5\n      1.0\n      0.000000\n    \n    \n      2.0\n      0.500000\n    \n    \n      6\n      1.0\n      0.532258\n    \n    \n      2.0\n      0.461538\n    \n    \n      7\n      1.0\n      1.000000\n    \n    \n      8\n      1.0\n      0.524590\n    \n    \n      2.0\n      0.338983\n    \n    \n      9\n      1.0\n      0.362637\n    \n    \n      2.0\n      0.354430\n    \n  \n\n\n\n\nWe can also visualize these trends using a barchart. From the barchart, we can see that there are not particularly large disparities in employment rate by gender for the different races, except for groups 5 and 7, which appear to only have data on females and males, respectively.\n\nimport seaborn as sns\n\nmeans = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index(name = \"mean\")\nsns.barplot(data = means, x = \"group\", y = \"mean\", hue = \"SEX\")\n\n<AxesSubplot: xlabel='group', ylabel='mean'>\n\n\n\n\n\n\n\n\nAfter doing some basic exploration of the data, it is now time for us to choose and train a model. We’ll train a logistic regression model, but we’ll tune the polynomial features parameter using our training data before testing our model.\n\n# utilized code from \"Introduction to Classification in Practice\" lecture\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\n\ndef poly_LR(deg):\n  return Pipeline([(\"poly\", PolynomialFeatures(degree = deg)),\n                   (\"LR\", LogisticRegression(penalty = None, max_iter = int(1e3)))])\n\n\n# utilized code from \"Introduction to Classification in Practice\" lecture\nfor deg in range(6):\n  plr = poly_LR(deg = deg)\n  cv_scores = cross_val_score(plr, X_train, y_train, cv=5)\n  mean_score = cv_scores.mean()\n  print(f\"Polynomial degree = {deg}, score = {mean_score.round(3)}\")\n\nPolynomial degree = 0, score = 0.542\nPolynomial degree = 1, score = 0.772\n\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nPolynomial degree = 2, score = 0.81\n\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nPolynomial degree = 3, score = 0.807\n\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nPolynomial degree = 4, score = 0.811\n\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nPolynomial degree = 5, score = 0.802\n\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nIt seems like performance peaks around features of degree 2, so we’ll use this to train our model.\n\nplr = poly_LR(deg = 2)\nplr.fit(X_train, y_train)\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nPipeline(steps=[('poly', PolynomialFeatures()),\n                ('LR', LogisticRegression(max_iter=1000, penalty=None))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('poly', PolynomialFeatures()),\n                ('LR', LogisticRegression(max_iter=1000, penalty=None))])PolynomialFeaturesPolynomialFeatures()LogisticRegressionLogisticRegression(max_iter=1000, penalty=None)\n\n\n\n\n\n\n\nAfter fitting our model, let’s first calculate its overall accuracy on the test dataset. We get an accuracy of 0.81. This is relatively good, but I would not want to release a tool into the real-world with an accuracy this low. It could be worth exploring other models or feature combinations for training a different model in the future.\n\nplr.score(X_test, y_test).round(4)\n\n0.8121\n\n\nNext let’s calculate positive predictive value (PPV) as well as false negative and false positive rates. To do this, we’ll extract our model’s predictions and construct a normalized confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = plr.predict(X_test)\nconf = confusion_matrix(y_test, y_pred, normalize = \"true\")\nconf\n\narray([[0.78817734, 0.21182266],\n       [0.15802469, 0.84197531]])\n\n\n\nFPR = conf[0,1]\nFNR = conf[1,0]\nPPV = conf[1,1]/np.sum(conf[:,1])\n\nprint(\"FPR: \", FPR)\nprint(\"FNR: \", FNR)\nprint(\"PPV: \", PPV)\n\nFPR:  0.21182266009852216\nFNR:  0.1580246913580247\nPPV:  0.7989912047831206\n\n\nWe see a PPV of 0.799, meaning that approximately 80% of the individuals that our model predicted were employed are actually employed. We see relatively low FPR and FNR, with the false positive rate about 30% higher than the false negative rate. This means our model overpredicts employed status - we are more likely to predict an unemployed person is employed, rather than predicting that an employed person is unemployed.\n\n\n\nNow let’s break things down by race, focusing specifically on the White and Black/African American groups. Looking at overall accuracy, we see that the accuracy for White indiciduals is slightly higher than Black/African American individuals, by about 3%.\n\ndf_test = pd.DataFrame(X_test, columns = features_to_use)\ndf_test[\"group\"] = group_test\ndf_test[\"label\"] = y_test\n\nblack_ix = df_test[\"group\"] == 2\nwhite_ix = df_test[\"group\"] == 1\n\ncorrect_pred = y_pred == df_test[\"label\"]\n\n# accuracy on Black defendants\naccuracy_black = correct_pred[black_ix].mean()\n\n# accuracy on white defendants\naccuracy_white = correct_pred[white_ix].mean()\n\nprint(\"White accuracy: \", accuracy_white)\nprint(\"Black accuracy: \", accuracy_black)\n\nWhite accuracy:  0.817117776152158\nBlack accuracy:  0.7911392405063291\n\n\n\nblack_conf = confusion_matrix(df_test[\"label\"][black_ix], \n                 y_pred[black_ix],\n                 normalize = \"true\")\nblack_conf\n\narray([[0.75621891, 0.24378109],\n       [0.14782609, 0.85217391]])\n\n\nNext we’ll break down PPV, FPR and FNR by group. We see that false positive rates are higher than false negative rates across both groups, but the gap between FPR and FNR is much larger for Black individuals (10% versus 4%).\n\nblack_FPR = black_conf[0,1]\nblack_FNR = black_conf[1,0]\nblack_PPV = black_conf[1,1]/np.sum(black_conf[:,1])\n\nprint(\"FPR: \", black_FPR)\nprint(\"FNR: \", black_FNR)\nprint(\"PPV: \", black_PPV)\n\nFPR:  0.24378109452736318\nFNR:  0.14782608695652175\nPPV:  0.7775628626692457\n\n\n\nwhite_conf = confusion_matrix(df_test[\"label\"][white_ix], \n                 y_pred[white_ix],\n                 normalize = \"true\")\nwhite_conf\n\narray([[0.79753762, 0.20246238],\n       [0.16037736, 0.83962264]])\n\n\n\nwhite_FPR = white_conf[0,1]\nwhite_FNR = white_conf[1,0]\nwhite_PPV = white_conf[1,1]/np.sum(white_conf[:,1])\n\nprint(\"FPR: \", white_FPR)\nprint(\"FNR: \", white_FNR)\nprint(\"PPV: \", white_PPV)\n\nFPR:  0.2024623803009576\nFNR:  0.16037735849056603\nPPV:  0.8057141441787311\n\n\n\n\n\n\n\nNow we’ll examine some specific bias measures. First let’s check if our model is well calibrated. To do this, we’ll check if the fraction predicted employed people who are actually employed is the same across groups. Of those predicted employed, slightly more White than black individuals were actually employed, but the rates are relatively consistent across groups.\n\ndf_test[\"pred\"] = y_pred\ndf_test = df_test[white_ix|black_ix]\n\nmeans = df_test.groupby([\"group\", \"pred\"])[\"label\"].mean().reset_index(name = \"mean\")\n\np = sns.barplot(data = means, x = \"pred\", y = \"mean\", hue = \"group\")\n\n\n\n\n\n\n\nTo determine error rate balance, we’ll check if FPR and FNR are consistent across both groups. We already calculated false positive and false negative rates by group above, and we can see that they differ slightly, but not substantially. We can plot these trends also to visualize them better. False negative rates are pretty consistent across the two groups, but the false positive rate for Black/African-American individuals is higher than for White individuals.\n\nrate_type = [\"FPR\", \"FNR\", \"FPR\", \"FNR\"]\ngroup = [\"Black\", \"Black\", \"White\", \"White\"]\nrate = [black_FPR, black_FNR, white_FPR, white_FNR]\n\nrates_df = pd.DataFrame(list(zip(rate_type, group, rate)), columns=['rate_type', 'group', 'rate'])\n\nmeans = rates_df.groupby([\"group\",\"rate_type\"])[\"rate\"].sum().reset_index(name = \"rate\")\np = sns.barplot(data = means, x = \"rate_type\", y = \"rate\", hue = \"group\")\n\n\n\n\n\n\n\nTo calculate statistical parity, we check if the proportion of individuals classified as employed is the same across groups. While the rates differ slightly, they are close enough that we can probably say statistical parity is satisfied.\n\nmeans = df_test.groupby(\"group\")[\"pred\"].mean().reset_index(name = \"proportion classified employed\")\nprint(means)\n\np = sns.barplot(data = means, x = \"group\", y = \"proportion classified employed\")\n\n   group  proportion classified employed\n0      1                        0.498903\n1      2                        0.465190\n\n\n\n\n\n\n\n\n\n\nAn employment prediction tool such as the model trained here could be useful for a variety of companies. For example, companies trying to recruit employees could use an employment prediction tool to find unemployed people to target for recruiting. Employment is also often used as a proxy for wealth/financial stability, so people trying to assess someone’s financial stability, such as a bank considering someone for a loan or a landlord considering someone for a lease might use an employment prediction tool when evaluating them. This could negatively impact people who were predicted as unemployed, particularly if they actually were employed, preventing them from needed opportunities for financial growth. Based on my bias audit, White individuals are more often predicted as employed than Black individuals. However, White individuals are less likely to be falsely predicted as employed than Black people are, while White people are slightly more likely to be falsely predicted as unemployed than Black people are. Thus, if my model was used as an employment prediction tool by banks, landlords, etc., it would probably approve slightly more White people for loans and leases than Black people. This could reinforce existing societal biases within the US which give White people more access to capital and opportunities for financial growth. On the other hand, the higher FPR for Black individuals could allow unemployed Black people to still have access to new sources of capital, housing, etc. If used as a recruiting tool, the higher FPR for Black individuals would intorduce additional hurdles for raising the employment rate among Black people because recruitment efforts would be more likely to be targeted towards White people.\nBased on my audit, I do not think that my model shows significant problematic bias. The model is relatively well-calibrated for race and it satisfies statistical parity. The one area where the model appears to be a bit biased is in error rate balance, specifically with regard to false positive rates. It is surprising to me that the FPR is higher for Black people than White people, as it is contrary to existing societal biases. While more balanced false positive rates would be preferred, a difference in 4% does not seem too problematic. Additionally, I think that it is better for this model to have inflated false positive rates, rather than false negative rates, as false positive rates would give people more opportunities to accrue capital, rather than less. Despite the model not showing much racial bias, I still do not think that employment prediction tools should be used. If a person is not required to divulge their employment status for a certain application process, then I do not think that their assumed employment status should be allowed to be considered. While employment prediction tools could be helpful for recruiting purposes, I think they are likely to do more harm than good in gatekeeping people from access to housing, money, and other living needs. I do not see enough positive uses for these tools that outweigh the potential negative impacts, so I think that their use should be banned."
  },
  {
    "objectID": "posts/logistic-regression/logistic-regression-blog.html#implementing-logistic-regression-with-gradient-descent",
    "href": "posts/logistic-regression/logistic-regression-blog.html#implementing-logistic-regression-with-gradient-descent",
    "title": "Logistic Regression Blog",
    "section": "Implementing Logistic Regression with Gradient Descent",
    "text": "Implementing Logistic Regression with Gradient Descent\nImplementing logistic loss with gradient descent does not differ hugely from implementing the perceptron algorithm, which I do in my last blog post. Both algorithms follow a similar formula of intializing random weights, and iteratively performing weight updates and calculating loss until the algorithm converges (or max iterations are performed). The main differences are the way that the weight update is performed and the function used to calculate loss. In this implementation, the weights are updated by calculating the gradient of the loss function with current weights, and then multiplying this by a specified learning rate and subtracting it from the current weights. Gradient descent converges when the gradient is zero, meaning a local minimum is found, and the loss stops updating. In this case, logistic loss was used. Logistic loss was chosen because it is a convex function, which means that we can treat local minima as global minima, making it easier to solve for the optimal weights. To train our logistic regression model, we follow a similar pattern of fitting the model to training data and calculating accuracy and loss as we train to see if the model is learning successfully.\n\nTesting the logistic regression model\nTo train and experiment on our model we must first create some data. Here we create data that is not quite linearly separable, but is relatively close.\n\nfrom logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAfter creating and visualizing some data, we fit it and visualize the loss and accuracy overtime.\n\nLR = LogisticRegression()\nLR.fit(X, y, .5, 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"loss\")\nplt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss/Accuracy\")\n\n\n\n\nWe see that as the loss decreases, the accuracy increases, which makes sense. If we run the model numerous times, we see that the amount of time it takes for the model to converge varies, which is largely a result of the initial random weight chosen.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3,3])\n\n(-3.0, 3.0)\n\n\n\n\n\n\nLR.score_history[-1]\n\n0.94\n\n\nIf we visualize the line drawn by our final weights, we see that it does a very good job of classifying our data. When we print the final accuracy we get 0.94, which seems reasonable for data that is not linearly separable. If we run these cells several times, we see the same line drawn each time, even if the evolution of the loss and accuracy looks different. This suggests that our logistic regression algorithm is always converging to the global minima, even if it doesn’t always take the same amount of time.\n\n\nChanging the learning rate\nWe can alter values of alpha, the learning rate, to make our algorithm converge more quickly. However, if we make our learning rate too large, our algorithm will never converge because it will jump over the minima each time it makes an update. To see this, lets set our learning rate to 50.\n\nLR = LogisticRegression()\nLR.fit(X, y, 50, 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history)\n#plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWe can see that when we do this, the algorithm never converges, but instead the loss bounces around between approximately 0.38 and 0.39 loss between epochs 100 and 1000.\n\n\nLogistic regression with stochastic gradient descent\nNext we implement logistic regression with stochastic gradient descent. This function uses the same principal of gradient descent, but it splits the data into batches and performs a weight update for each batch before updating the loss and score. By performing multiple updates within each epoch, the algorithm will likely converge more quickly.\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 1000, 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history)\n#plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nplt.loglog()\n\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3,3])\n\n(-3.0, 3.0)\n\n\n\n\n\n\nLR.score_history[-1]\n\n0.935\n\n\nOur stochastic algorithm also draws the same line, although we need to use a smaller learning rate to allow the algorithm to converge, and we see small fluctuations at higher epochs.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWhen we compare the stochastic versus regular gradient descent, we can see that the stochastic loss decreases more quickly at the beginning, but we see small fluctuations in loss at later timesteps.\nNow lets compare a series of batch sizes and see how it affects how quickly the algorithm converges.\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 100, 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size = 10\")\n#plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 100, 20)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size = 20\")\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 100, 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size = 50\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nxlab = plt.xlabel(\"Epochs\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWe can see that as we increase the batch size, the algorithm takes longer to converge. This makes sense, as the power of stochastic gradient descent lies in performing several updates within each epoch. As we increase the batch size, we decrease the number of updates performed in each epoch, requiring more epochs for the algorithm to converge."
  },
  {
    "objectID": "posts/linear-regression/linear-regression-blog.html#implementing-linear-regression",
    "href": "posts/linear-regression/linear-regression-blog.html#implementing-linear-regression",
    "title": "Linear Regression Blog",
    "section": "Implementing Linear Regression",
    "text": "Implementing Linear Regression\nIn this blog post I implement linear regression in two ways: analytically, using an equation that solves for the optimal model weights, and using gradient descent, similar to the methods used in my prior logistic regression blog post. The implementation of linear regression is similar to logistic regression, but rather than calculating binary labels for a given data point, it calculates a specific value based on a data point’s features. This is what differentiates regression tasks (predicting a number) from classification tasks (predicting a label). To calculate optimal weights analytically, the following formula was used (taken from lecture notes): \\[\\mathbf{\\hat{w}}=\\mathbf{(X^TX)^{-1}X^Ty}.\\] When implementing linear regression with gradient descent, the gradient was calculated with (from blog assignment description): \\[\\mathbf{\\nabla L(w)}=\\mathbf{X^T(Xw-y)},\\] and used to perform weight updates as done when using gradient descent for logistic regression. Both methods produced the same weights and were able to achieve relatively high accuracies on training and validation data. Experiments were done to analyze overfitting as the number of features were increased, and LASSO regularization was employed to minimize overfitting with increasing features. Lastly, I use my linear regression model to predict trends in casual bikeshare usage in Washington DC, finding that my model is able to learn trends in bikeshare usage relatively well.\n\nTesting Two Implementations\nBefore testing my two fitting techniques, I have to generate some data. I have generated both training and validation data, visualized below. We can see the two datasets exhibit similar trends, so my model trained on the training data, should generalize to the validation data relatively well.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNext I import the model and test it, first fitting the data using the analytical technique desribed above. When we print the accuracies, we see a training score of 0.59 and a validation score of 0.62. The model actually performs better on the validation data! This is encouraging, as it means we are likely not overfitting.\n\nfrom linear_regression import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.5944\nValidation score = 0.6242\n\n\nWe can visualize the results of our model, and we see that it does a good job of drawing a line that reflects trends in the data.\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[0].plot(X_train, LR.predict(X_train), color = \"black\")\naxarr[1].scatter(X_val, y_val)\naxarr[1].plot(X_val, LR.predict(X_val), color = \"black\")\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nWhen we print the weights of the model fit using analytical methods and compare it to the model weights when using gradient descent, we see that we get the same weights, as we would expect. If we trained the gradient descent model on more epochs, we would likely see even more agreement in the later decimal places.\n\nLR.w\n\narray([1.09501024, 0.88755776])\n\n\n\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method = \"gradient\", alpha = 0.01, max_epochs = 1e2)\nLR2.w\n\narray([1.09428466, 0.88794095])\n\n\nWe can also plot the score history of the gradient descent model and we see an initial steep increase in the score, as expected, before plateauing around 0.7 after about 20 iterations.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n\nExperimenting with Number of Features\nNext we increase the number of features and see how this impacts model performance on both training and validation data. We see that as we increase the number of features, the training score also increases before plateauing around 1.0, the highest possible value. The validation score initially increases as well, but then sharply declines to values below zero, indicating very poor performance, as the number of features approaches the number of data points. This is due to overfitting. As we increase the number of features to get closer to the number of data points, the model has a difficult time generalizing and extracting patterns because there is not enough data. This negatively impacts the validation score because rather than extracting general patterns, the model simply learns the exact behavior of the training dataset.\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrain_scores = []\nval_scores = []\nLR3 = LinearRegression()\n\nfor i in range(1,n_train):\n    p_features = i\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR3.fit(X_train, y_train)\n    train_scores.append(LR3.score(X_train, y_train).round(4))\n    val_scores.append(LR3.score(X_val, y_val).round(4))\n    \nplt.plot(np.arange(1,n_train),train_scores, label = \"training\")\nplt.plot(np.arange(1,n_train),val_scores, label = \"validation\")\n\nlabels = plt.gca().set(xlabel = \"Num features\", ylabel = \"Score\")\n\nlegend = plt.legend()\n\n\n\n\nTo combat this, we utilize LASSO regularization. LASSO regularization constrains the weight vector to be small, forcing many weights to be zero. This minimizes the effects of overfitting because it constrains the number of features that actually contribute to the model’s final predictions. When we employ LASSO regularization with an alpha of 0.001 we see a dip in later validation scores, but it is much smaller, dipping from about 1.0 to 0.8, rather than falling to almost -0.75.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrain_scores = []\nval_scores = []\nL = Lasso(alpha = 0.001)\n\nfor i in range(1,n_train):\n    p_features = i\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    train_scores.append(L.score(X_train, y_train).round(4))\n    val_scores.append(L.score(X_val, y_val).round(4))\n    \nplt.plot(np.arange(1,n_train),train_scores, label = \"training\")\nplt.plot(np.arange(1,n_train),val_scores, label = \"validation\")\n\nlabels = plt.gca().set(xlabel = \"Num features\", ylabel = \"Score\")\n\nlegend = plt.legend()\n\n\n\n\nExperimenting with different alpha values, we see that when we increase alpha to 0.01 the score dips further. When we decrease alpha to 0.0001, we see similar behavior to our original alpha value of 0.001, with a slightly smaller dip.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrain_scores = []\nval_scores = []\nL = Lasso(alpha = 0.01)\n\nfor i in range(1,n_train):\n    p_features = i\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    train_scores.append(L.score(X_train, y_train).round(4))\n    val_scores.append(L.score(X_val, y_val).round(4))\n    \nplt.plot(np.arange(1,n_train),train_scores, label = \"training\")\nplt.plot(np.arange(1,n_train),val_scores, label = \"validation\")\n\nlabels = plt.gca().set(xlabel = \"Num features\", ylabel = \"Score\")\n\nlegend = plt.legend()\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\ntrain_scores = []\nval_scores = []\nL = Lasso(alpha = 0.0001)\n\nfor i in range(1,n_train):\n    p_features = i\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    L.fit(X_train, y_train)\n    train_scores.append(L.score(X_train, y_train).round(4))\n    val_scores.append(L.score(X_val, y_val).round(4))\n    \nplt.plot(np.arange(1,n_train),train_scores, label = \"training\")\nplt.plot(np.arange(1,n_train),val_scores, label = \"validation\")\n\nlabels = plt.gca().set(xlabel = \"Num features\", ylabel = \"Score\")\n\nlegend = plt.legend()\n\n/Users/elizawieman/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.029e-01, tolerance: 4.254e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\nAnalyzing Trends in Bikeshare Usage with Linear Regression\nLastly, we use our linear regression model to predict trends in the casual usage of bikeshare programs in Washington DC. First we import the data, visualize trends, and do some data cleaning in preparation for model training and prediction.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\nFirst let’s visualize the data. We see the number of casual users fluctuate seasonally, as well as within each month, perhaps depending on weather patterns or time of week.\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nNext we extract relevant information for our model and split our dataset for testing and training.\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\nFinally we fit our model, and visualize the test predictions alongside the actual data. We see that our model is able to reproduce the general trends in casual usage, although tends to predict smaller fluctuations in usage (lower maxima, higher minima) than the actual data shows.\n\nLR4 = LinearRegression()\n\nLR4.fit(X_train, y_train)\nLR4.score(X_test, y_test)\n\ny_pred = LR4.predict(X_test)\n\n\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(np.arange(len(y_test)), y_test, label=\"test\")\nax.plot(np.arange(len(y_test)), y_pred, label=\"pred\")\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\nlegend = plt.legend()\n\n\n\n\nFinally, we can examine which factors the model considers to be contributing the most to usage. The highest weight in our model contributes to the variable temperature, meaning that our model considers temperature to be a very important factor in determining casual bike usage. This makes sense, as people would probably be more likely to ride a bike on a warmer day. The weight corresponding to windspeed is the most negative value by far. This also makes sense, as a high windspeed would likely discourage people from wanting to bike outside. We also see trends by month, with higher weights attributed to the spring and fall months, some of the nicest bike weather, and lower weights attributed to summer and winter months, when it could be to hot or cold to ride comfortably. We also see that people are less likely to use bikeshares on working days and holidays, suggesting that much of the casual bikeshare usage happens on the weekends.\n\nprint(LR4.w)\nprint(X_train.columns)\n\n[ -108.37113627  -791.69054913   280.58692733  1498.71511272\n  -490.10033978 -1242.80038075  -235.87934918    -3.35439712\n   369.27195552   518.40875345   537.30188616   360.80799815\n   228.88148125   241.31641202   371.50385387   437.60084787\n   252.43300405    90.8214605    919.07676215]\nIndex(['weathersit', 'workingday', 'yr', 'temp', 'hum', 'windspeed', 'holiday',\n       'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8',\n       'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12'],\n      dtype='object')"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementing-perceptron",
    "href": "posts/perceptron/perceptron_blog.html#implementing-perceptron",
    "title": "Perceptron Blog",
    "section": "Implementing Perceptron",
    "text": "Implementing Perceptron\nThe perceptron update was implemented using: \\[\\mathbf{\\tilde{w}}^{(t+1)}=\\mathbf{\\tilde{w}}^{(t)}+\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)y_i\\mathbf{\\tilde{x}}_i\\] (Equation 1 in https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-perceptron.html#source-code). This update works by calculating the dot product of the current weights and the features of the selected point and then multiplying it by the true label of that point. If the signs of the dot product and the label are different then the weights are updated, otherwise the weights remain the same. In the case that an update does occur, the new weights are calculated by multiplying the features of the selected point by the actual label and adding that to the current weights. Since we multiply the selected points features’ by the point’s true label, we are always moving our weights in the direction of that label, improving the perceptron’s ability to classify that specific point. By iterating through the points randomly and performing updates, the perceptron algorithm eventually converges if the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#testing-the-algorithm",
    "href": "posts/perceptron/perceptron_blog.html#testing-the-algorithm",
    "title": "Perceptron Blog",
    "section": "Testing the Algorithm",
    "text": "Testing the Algorithm\nThree experiments were performed to verify that the perceptron algorithm was implemented correctly.\n\nExperiment 1: Linearly Separable Data\nFirst the perceptron was fit on linearly separable data with two features.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n100 data points were generated, each with two features and a label of 1 or 0. The data was designed to be linearly separable (meaning the perceptron should converge if run for long enough).\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe algorithm was fit to the data with a maximum of 1000 iterations.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nIf we print the weight vector, we see three values. The first value is the weight of feature 1, the second is the weight of feature 2, and the third is the bias.\n\np.w\n\narray([2.10557404, 3.1165449 , 0.25079936])\n\n\nWe can also plot the accuracy as the model is fit.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIf we print the accuracy, we see that it is 1.0, we classified all of our data correctly!\n\np.score(X, y)\n\n1.0\n\n\nWe see this too when we draw the line specified by our weights. The line perfectly separates the two types of data.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-4,4])\n\n(-4.0, 4.0)\n\n\n\n\n\n\n\nExperiment 2: Not Linearly Separable Data\nNext let’s see how our algorithm runs on data that isn’t linearly separable. First we’ll define some new data. We still have 100 points, each with two features and a label, but when we visualize the data below we can see that it is not clustered as nicely.\n\nnp.random.seed(123)\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nA new perceptron was fit and the weights were printed. Like with Experiment 1, we see a weight vector with three values.\n\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\np2.w\n\narray([ 3.25206473,  1.64360407, -1.10792451])\n\n\nWhen we plot and print the accuracy over time, we see that we were able to achieve an accuracy of 0.89 after 1000 runs. This makes sense, as an accuracy of 1.0 is required for the algorithm to end early, which is impossible on this dataset.\n\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X,y)\n\n0.89\n\n\nWhen we visualize the line that the perceptron specified, we see that it’s pretty good. It seems unlikely that the accuracy could get much better on this dataset.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3.5,3.5])\n\n(-3.5, 3.5)\n\n\n\n\n\n\n\nExperiment 3: Data with More than 2 Features\nFor the last experiment we see if the perceptron can classify data with more than two features. I generated a dataset of 100 points, still with binary labels, but this time each data point has five features.\n\nX, y = make_blobs(n_samples = 100, n_features = 5, centers = 2)\n\nNow when we fit our perceptron and print the weights we get a six element vector, where the first five values correspond to the features and the final value corresponds to the bias. Our perceptron generalizes to data with more than two features!\n\np3 = Perceptron()\np3.fit(X,y,max_steps = 1000)\np3.w\n\narray([ 2.02291666,  6.08425872,  0.74441411,  5.17387787, 10.87406143,\n       -1.69055338])\n\n\nWhen we plot and print the accuracy of our perceptron we see that we achieve a loss of 1.0. This suggests that the dataset I generated is linearly separable.\n\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X,y)\n\n1.0"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#runtime-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#runtime-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Runtime of the Perceptron Update",
    "text": "Runtime of the Perceptron Update\nEach perceptron update (see Equation 1) is performed on only one data point, so the runtime of a single update is independent of the number of data points in the provided dataset. Performing the dot product \\(\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle\\) requires p+1 multiplications and p additions, for a runtime of O(p) (where p = number of features). The remaining operations within the expression \\(\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)\\) are scalar multiplication or boolean comparison, so the total runtime of this expression is O(p), dominated by the dot product. This expression produces a scalar which is multiplied by \\(y_i\\), another scalar, and \\(\\mathbf{\\tilde{x}}_i\\), which has dimensions (p+1)x1. Multiplying two scalars by a vector with dimensions (p+1)x1 takes O(p) time, so the runtime of \\(\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)y_i\\mathbf{\\tilde{x}}_i\\) is O(p+p). The result of this expression is then added to the prior weight vector, which requires p+1 additions. Thus the total runtime of the update is O(p+p+p) = O(3p), which simplifies to O(p), a linear runtime determined by the number of features."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html",
    "href": "posts/final proj/Final Blog Post.html",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "",
    "text": "Source code: https://github.com/ebwieman/wildfire-risk-tool/blob/main/main.ipynb\nMapping tool: https://github.com/ebwieman/wildfire-risk-tool/blob/main/wildfire_risk_mapping_tool.ipynb"
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#abstract",
    "href": "posts/final proj/Final Blog Post.html#abstract",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Abstract",
    "text": "Abstract\nWildfires have increased in recent years as a result of climate change, posing a threat to humans and the environment. Understanding the conditions that lead to wildfire and which areas are most at risk is important for developing mitigation strategies and allocating resources. In this project, we utilize machine learning algorithms to predict both wildfire occurrence and area of wildfires. To predict wildfire occurrence, we trained several machine learning models on a dataset containing meteorological information about potential wildfires in Algeria. An accuracy of 100% was achieved using a logistic regression model trained on all features. To predict wildfire area, models were trained on a dataset of wildfire events from Montesinho National Park, Portugal. The area prediction task was much more difficult than the classification task and required additional label transformation to achieve higher accuracies. The highest area prediction accuracy was achieved with a logistic regression model trained on all features and using labels transformed with the scikit-learn lab encoder. Lastly, a model trained on the Algerian dataset was used to predict and map wildfire risk in the United States. The trained model was more simplistic than other models due to the lack of US meteorological data available, but visual comparison to existing fire prediction tools such as the USGS Fire Danger Map Tool shows that the model was somewhat able to predict fire risk in the United States. One shortcoming of this work is that all datasets used to train our models were small and regionally specific, making it difficult to create generalizable tools for use on larger scales or different regions. Developing larger and more regionally dispersed wildfire datasets will aid in future creation of more robust fire prediction tools."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#introduction",
    "href": "posts/final proj/Final Blog Post.html#introduction",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Introduction",
    "text": "Introduction\nClimate change has increased the likelihood of a variety of extreme weather events, including wildfires. These extreme events pose definite risks to both human and ecological communities. At the same time, machine learning is emerging as an important predictive tool in natural disaster prevention; numerous studies have already used machine learning to classify risk of natural hazards. For example, Youssef et al. used machine learning models to predict an area’s susceptibility to landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023). This study experimented with a few different models, ultimately settling on Random Forest. In Choubin et al.’s study of avalanches in mountainous regions, Support Vector Machine (SVM) and Multivariate Discriminant Analysis were found to be the best models in assessing avalanche risk based on meteorological and locational data (Choubin et al. 2019).\nPrior studies have also focused specifically on wildfire prediction. A 2023 paper even developed a new model to better predict burned areas in Africa and South America (Li et al. 2023). In addition, the dataset used in our project to predict Portuguese fires was originally the topic of a 2007 paper focusing on training models on readily available meteorological data to predict the area of wildfires (Cortez and Morais 2007). This study also experimented with different predictive features and models, finding SVM and random forest to be the best predictors (Cortez and Morais 2007).\nIn this project, we build on these prior studies, using many similar techniques and models. We use two datasets to predict wildfire likelihood and wildfire area given meteorological data. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire events with 11 attributes that describe weather characteristics at the time of that event. Our second dataset, the Forest Fires Dataset, contains data from Montesinho National Park, Portugal. This dataset contains many of the same weather characteristics as the Algerian dataset, but contains only fire events, and also includes the areas of these events. Rather than using this dataset to predict whether a fire occurred in a given area, we used it to predict the size a fire is likely to reach given certain meteorological conditions.\nOver the course of this project, we followed in the footsteps of prior studies, experimenting with different models to predict risk. We implemented several existing machine learning models such as Random Forest and Logistic Regression, ultimately choosing the models and sets of features that yielded the highest accuracy score. We trained and validated models on the Algerian dataset to predict whether or not a forest fire will occur given certain meteorological conditions and also trained/validated models on the Portugal dataset to predict forest fire area given many of the same features.\nAlthough we trained our models with localized data, we also assessed the extent to which we could apply our models across datasets and geographies. Similarly to the multi-hazard susceptibility map produced by Youssef et al., we created a fire susceptibility map of fire risk using our models and county-level temperature and precipitation data for the entire United States (Youssef et al. 2023). While we cannot assess the accuracy of this map directly, we can compare it to existing US wildfire prediction tools to at least visually assess our accuracy.\nFinally, we build on existing research to assess the ethics of using machine learning models in predicting risk of natural hazards. As Wagenaar et al. caution in their 2020 study of how machine learning will change flood risk and impact assessment, “applicability, bias and ethics must be considered carefully to avoid misuse” of machine learning algorithms (Wagenaar et al. 2020).\nAs extreme weather events become more prevalent with climate change, we must learn how to best predict and manage these events. The methods explored detail our own attempt to do so."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#values-statement",
    "href": "posts/final proj/Final Blog Post.html#values-statement",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Values Statement",
    "text": "Values Statement\nBeing able to accurately predict the risks of wildfires offers numerous benefits for various stakeholders. Predictive models like ours have the potential to revolutionize wildfire management and mitigation strategies, leading to better preparedness, timely response, and ultimately, the protection of lives and property. Thus, we wanted to build upon prior studies and further the ability to accurately predict wildfires.\nThe main beneficiaries of this project are the wildfire departments and emergency responders. These agencies can gain valuable insights into the probability and severity of wildfire occurrences in specific areas though our project. These predictions allow for better allocation of resources and more effective mitigation strategies, reducing the overall damage and saving tons of valuable resources. If we can assess areas with high-risk for wildfires ahead of time, they could be better equipped and protected.\nInsurance companies can also leverage our predictive models to assess risks associated with wildfires. By integrating these algorithms into their underwriting processes, insurers can accurately evaluate the potential impact of wildfires on properties, allowing for more precise risk assessment. Moreover, the models can aid in post–fire analysis, enabling insurance companies to provide timely assistance to policyholders living in at-risk areas.\nGovernment agencies responsible for land and forest management are also another one of our potential users. Policymakers can make more informed decisions regarding land use, forest management, and allocation of resources for fire prevention efforts. Ultimately, this can lead to more proactive measures such as improved firebreak construction and targeted vegetation management.\nAnother group that may not directly utilize our predictive wildfire models, but still stands to benefit is the general population. Information that trickles down from fire management authorities can help individuals living in fire-prone areas make informed decisions and take the necessary precautions. People are able to take property protection measures and implement evacuation plans they deem necessary. Overall, the predictions from these models can increase safety measures while minimizing property damage within the predicted at-risk areas.\nWhile the use of machine learning models to predict wildfire risks can be highly beneficial, there are potential downsides with their implementation that should be considered. Our models rely heavily on well documented weather and geographical data. Data collection takes time and money, which could potentially be a barrier to using our models. Remote areas without government or research funding most likely will not be able to produce the data needed to benefit from our models. Moreover, communities lacking internet access, computing devices, or technological literacy are unable to take advantage of our models. This can disproportionately affect rural areas and low-income communities, further exacerbating existing inequalities.\nAdditionally, we recognize that our models are trained and tested on data that is in English. This language barrier can hinder individuals who don’t have the proficiency in the language, limiting their ability to use these predictive models. Additionally, cultural differences and contextual nuances might not be well captured in our models, leading to potential misunderstandings and biases. Thus, we want to be mindful of the potential barriers and hope to address these shortcomings in our future work. Failing to address these disparities could perpetuate social inequalities in wildfire management.\nUpon reflecting these potential harms and benefits, we still believe this project will improve wildfire management and be a crucial resource to communities in fire-prone areas. Additionally, this project furthers our understanding of factors contributing to wildfires. With this information, we can accurately predict the likelihood of wildfires in a given region, enabling better fire management, mitigation, and evacuation. Wildfire predictions can help protect the land, wildlife, and local communities. Still, efforts should be made to actively involve marginalized groups in wildfire preparedness and response initiatives."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#materials-and-methods",
    "href": "posts/final proj/Final Blog Post.html#materials-and-methods",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\nThe Datasets\nIn this project, we utilized various machine learning models from scikit-learn to better understand wildfire occurrences. Specifically, we trained and tested on two datasets to predict wildfire likelihood and wildfire area given the meteorological and geographical features. These datasets were taken from the University of California Irvine Machine Learning Repository. Our first dataset, the Algerian Forest Fires Dataset, contains fire and non-fire occurrences with 11 attributes describing weather characteristics at the time the event occurred. Our second dataset, the Forest Fires Data, has data from Montesinho National Park, Portugal. This dataset contains many of the same weather attributes as the Algerian one. However, rather than using this dataset to predict whether a fire occurred in a given area, it can be used to predict the size of the fire given certain meteorological and geographical conditions.\nThe Algerian dataset has 224 instances that regrouped data from two regions in Algeria, namely the Bejaia region located in the northeast of Algeria and the Sidi Bel-abbes region in the northwest of Algeria. There are 122 instances for each of the two regions. This data was collected from June 2012 to September 2012. As stated, the dataset contains fire and non-fire events with 13 attributes that describe weather conditions. The 13 features are: day, month, year, temperature in Celsius, relative humidity, wind speed, rain, fine fuel moisture code, duff moisture code, drought code, initial speed index, build up index, and fire weather index. Our target label is “fire”, with a label of 1.0 meaning that a fire has occurred and 0.0 being no fire.\nThe data collected in the Portuguese dataset was taken from Montesino park in the northeast region of Portugal. The 12 features in this dataset are similar to the ones in the Algerian dataset. These features include: x-axis spatial coordinate, y-axis spatial coordinate, month, day, fine fuel moisture code, duff moisture code, drought code, initial speed index, temperature in Celsius, relative humidity, wind, and rain. The target label is “fire area”, which determines the total burned area of a fire given the meteorological and geographical conditions.\nWe used RidgeCV from scikit-learn to pull out five important features in each dataset that we later trained on. RidgeCV is a cross validation method in ridge regression. The higher absolute coefficient, the more important that feature is. After applying RidgeCV to the Portuguese dataset, we yielded the five most important features: x-axis spatial coordinate, y-axis spatial coordinate, initial speed index, temperature, and wind. The five most important features for the Algerian dataset are: relative humidity, rain, fine fuel moisture code, initial speed index, and fire weather index.\n\n\nModeling\nBoth datasets were trained on various algorithms taken from scikit-learn and each model was analyzed based on their accuracy score.\nThe Algerian training data was trained on four different algorithms: decision tree, logistic regression, random forest, and stochastic gradient descent. For each of the algorithms, we trained the Algerian training set with its complete set of 13 features and also with the 5 features selected via our feature selection process. Additionally, we used cross validation to evaluate the models’ performance at different settings and to avoid overfitting. All five algorithms performed relatively well on the training data, with the accuracy score ranging from 0.552 to 0.990. Training the complete set of features on the stochastic gradient descent classifier yielded the lowest score of 0.552. Alternatively, training on the logistic regression classifier with the complete set of features gave us the highest accuracy of 0.990. Therefore, we went ahead and used the logistic regression model to test our Algerian data on.\nAs for the Portuguese dataset, we started by training it on the linear regression model and support vector machine for regression, but got very low accuracy scores of 0.048 and -0.026, respectively. It was much harder to train on this dataset because the labels were non-binary and many were skewed towards 0’s. We then performed a log-transform on our labels, hoping to improve the accuracy. However, this did not make much of a difference. The highest score we got after log-transforming our labels and training on a linear regression model is -0.020. After this, we went ahead and transformed our labels through three different methods. First, we used the lab encoder in the scikit-learn preprocessing package to encode the target labels with values between 0 and n number of classes - 1. Second, we divided the labels into 50 different ranges. Last, we transformed the labels into binary labels, making the areas larger than 0.0 as “non 0’s” and keeping labels of 0.0 as “0.0”. While we recognize that this method is not ideal, it was very difficult training the Portuguese dataset since it is a very small dataset and most of the target labels are skewed towards 0.0.\nIn the end, we trained our dataset on four different models: linear regression, logistic regression, support vector machine for classification, and support vector machine for regression. Each model was trained on different combinations of features and transformed target labels. We trained the model on its complete set of features, on the features selected via our feature selection process, and on the features highlighted in the research paper written by (Cortez and Morais 2007). In their paper, they also experienced difficulty training this Portuguese dataset. Ultimately, transforming our labels via the lab encoder got us a training score of 0.513 when trained on a logistic regression model using the complete set of features. Unsurprisingly, changing our y into binary labels and into ranges got us a perfect training accuracy of 1.0. Thus, we decided to test our Portuguese data on the logistic regression model with the transformed target labels.\n\n\nUS Meteorological Data and Mapping Tool\nCounty level precipitation and temperature data was downloaded from the NOAA NClimGrid dataset (Durre 2023). Specific months of interest were selected and data was downloaded for that month as two csvs, one containing precipitation data and one containing temperature data. The precipitation data contained total precipitation in mm for each day of the month for each US county, while the temperature data contained maximum temperature in Celsius for each day. Significant time was spent searching for additional meteorological data, specifically wind speed and humidity, but these efforts were ultimately unsuccessful. The temperature and precipitation datasets were uploaded to GitHub and then loaded into the Mapping Tool Jupyter notebook. Substantial data cleaning was performed to prepare the US data for prediction. Data cleaning tasks included renaming columns after the datasets were read in and resolving discrepancies in the state and county codes so that data frames could be merged by county. The precipitation and temperature datasets were read in, converted to pandas dataframes, and merged. From this dataframe, data was extracted for a specified day and county-level wildfire predictions were generated for that day. Predictions were generated using a new Random Forest model that was trained on the Algeria dataset using only Temperature and Rain as features, as these were the only features we had access to for US data. These predictions were then joined with a dataframe containing geological coordinates for each county and then the predictions were mapped. The final algorithm used for mapping allows the user to put in a date and state and then produces a map of county-level fire risk for that state on the specified day. If there is no data for the specified day, the algorithm returns a message apologizing for the lack of data. If no state is specified, the algorithm returns a map of the entire continental US. While the maps could not be rigorously assessed for accuracy, visual comparison to the USGS Fire Danger Map Tool was used to discuss the accuracy of our Algerian model when applied to US data (Eidenshink 2023)."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#results",
    "href": "posts/final proj/Final Blog Post.html#results",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Results",
    "text": "Results\nWe came close to achieving all of our goals for this project. Our greatest success was our modeling of the Algerian dataset. We achieved high training accuracy with several models, including > 95% training accuracy for a Random Forest Classifier, Logistic Regression, and Decision Tree Classifier, as well as 100% testing accuracy with a Logistic Regression model. It’s difficult to say how applicable this model is outside of the region of Algeria the data are from. The random forest classifier that we trained on the Algerian model for our mapping tool got many things right – it predicts more widespread fire in the summer months, for example, than in the winter – but we trained it on many fewer features than are in the full Algerian dataset.\nThis is certainly one of the shortcomings of our project. The lack of weather data available for the United States means that we can’t fully say how well our model would perform on data from a region other than Algeria. However, there are positive trends, and were more weather observations available, it seems likely that our model would have some applicability.\nDespite the fact that we did not get to realize the full abilities of our model, creating an interactive map is nonetheless a highlight of the project. It shows that our project has the ability to be broadly applicable, and underscores the power of Machine Learning as a tool for natural hazard prediction and prevention. We explored the ethical dimensions of this application of Machine Learning in a short essay, and while there are certainly ethical considerations that must be made when training models to predict natural hazards, this project shows that there is also a great deal of opportunity to use Machine Learning to predict and manage risks from natural hazards.\nWhile we cannot assess the accuracy of our US prediction tool directly, visual comparison to existing mapping tools such as the USGS Fire Danger Map Tool yields some insights. If we compare the two tools’ predictions for March 2, 2023, we see that our tool vastly overpredicts fire risk in the US (Figure 1). Our tool correctly predicts fire in the locations that the USGS identifies the most high risk, specifically Southwestern Texas and Arizona, but we also predict risk of fire in large swaths of the Midwest and Florida that the USGS tool does not deem particularly high risk.\n \n\nFig 1: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for March 2, 2023.\n\nNext we look at a potentially more interesting case in July 2020, the peak of summer, when we would expect more areas of the country to be susceptible to wildfires. Here the USGS predicts risk of fire in much of the Southwestern United States, and we see that our model does as well. However, we once again see overprediction by our model, specifically in the Midwest. One possible contributing factor is that the USGS model differentiates between agricultural land and does not predict fire risk in these areas. Our model seems to predict fire risk in many of the areas deemed agricultural by the USGS model, so differentiating between land cover classes could be useful for future development of this tool. Additionally, our model predicts on the county scale, while the USGS mapping tool appears to have much better resolution, allowing it to make more refined predictions. One other exciting observation in the July maps is that while our model tends to make generally uniform predictions across states, we do see some agreement between the two tools in identifying pockets of low fire risk within the Southwestern United States. The fact that our mapping tool tends to make relatively uniform predictions across states suggests that the model could just be learning general weather patterns rather than actually learning fire risk. Training on additional features would likely help address this problem, but this is impossible at the moment due to the lack of aggregated US weather data available.\n \n\nFig 2: Comparison of fire risk predictions by our mapping tool (top) and the USGS Fire Danger Map Tool (bottom) for July 2, 2020.\n\nThe final area of our project to discuss is the models we trained for the Portuguese dataset. Replicating the process of the original academic paper didn’t yield results of the same accuracy as those obtained by the study (Cortez and Morais 2007). Log-transforming the data was a complex process that was perhaps more involved than we had anticipated. However, we still managed to achieve about 50% accuracy with both Logistic Regression and Support Vector Machine models, and 100% accuracy when we transformed to binary labels and categorical labels. Figure 3, shown below, shows the results the model achieved on 20 features from the test set:\n\n\n\nimage\n\n\n\nFig 3: While the model correctly predicts 35% of fire areas, it gets close on a number of them, so 35% is not reflective of the actual performance. We see that the model does the best at predicting small fires, though for medium-sized fires, it usually either far overshoots or far undershoots in its prediction.\n\nFigure 4 shows the same observations, predicted using the SVC model. It also gets 35% correct, but unlike the LR model, it only ever underestimates the area of fires, while the LR model overestimates several. This suggests that the SVC model is slightly more conservative, and may fail to predict the largest fires.\n\n\n\nimage\n\n\n\nFig 4: The SVC model seems to be more conservative, predicting the area of all fires – even those that are larger in size – to be 0 ha or close to it.\n\nThese mixed results for the Portuguese data, along with our high testing accuracy for the Algeria dataset, shows that predicting whether or not a fire occurred is a much more straightforward task than predicting the area of the fire. There is certainly room to grow in our modeling of this dataset; perhaps trying different models or features would yield different results. We recognize that transforming the target labels into categorical and binary labels isn’t ideal; however, it was too difficult otherwise. This was because the Portuguese dataset is a very small dataset with the majority of the target labels skewed towards 0.0. Thus, this problem was turned into a classifying problem instead of producing an accurate predictor model. Nonetheless, our process and results show that it is possible to build a model trained on readily-available weather observations that predicts the area of fires with a reasonable degree of accuracy."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#concluding-discussion",
    "href": "posts/final proj/Final Blog Post.html#concluding-discussion",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOverall, our project was successful in a number of ways, and opens the door for future research and experimentation. Our analysis of wildfire risk provided us with an opportunity to practice data science. We successfully found regional data sets, cleaned them, and prepared them for machine learning analyses. Using a Ridge model, we successfully selected features to use for making predictions in each dataset, and through experimenting with a variety of different machine learning models, we selected two that reached reasonably high accuracy. Through examining United States county-level data, we were able to apply our models across scales. Finally, we visualized our results well, and effectively discussed their ethical implications.\nAs a group, we met and even surpassed our original project goals. We have a code repository that details our data preparation and cleaning, model training and prediction, and evaluation, notebooks with in exploratory data analysis and testing of our models, a short essay about the implications of using machine learning and automated tools for ecological forecasting (both risks and benefits) and a map of wildfire risk in the United States constructed using our models and US meteorological data. We had wanted to use machine learning to help with natural hazard risk detection and prevention, and our final product provides detailed and thorough documentation of our attempt to do so.\nBecause our project built on the work of scholars such as Cortez and Morias, we have the ability to compare our results to past work. While Cortez and Morias’ 2007 study that explored the Portugal data we used in our project identified SVM to be the best machine learning model for fire area prediction, we found SVC and Logistic Regression to be equally effective on the Portugal data set (Cortez and Morais 2007). Our work weaves in well with prior studies, allowing us to make predictions about natural risk by choosing and applying machine learning models. Overall, through experimenting with different models, we ultimately were able to predict wildfires and wildfire area more accurately than our base rates, and experimented with applying these models across scales. With more time, data, or computational resources, we could improve our methods and findings. Spending more time finding and downloading more data for the United States (such as wind speed and humidity, prehaps) would likely improve our map and our model and allow us to better apply the model across space. While we trained our machine learning models on regionally-specific data, we applied thier predictions to new and differently-scaled geographies. When applying models across space, it is important to do so in collaboration those people who may be affected by the results of predictive machine learning tools (Wagenaar et al. 2020). Overall, when we pay attention to bias and applicabiltiy, wildfire and natural disaster models have numerous applications, and even the potential to save lives. As the likelihood and intensity of extreme weather events continues increasing in the face of climate change, we should incorporate and continue building on the anaysis presented here to acheive even better methodologies to predict and prepare for these events."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#group-contributions-statement",
    "href": "posts/final proj/Final Blog Post.html#group-contributions-statement",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nMadeleine: Near the beginning of the project, Wright and I worked on preparing the data from the Algeria and Portugal data sets to eventually split into testing and training data. I then worked on selecting features important for fire/fire area prediction using RidgeCV. We then worked on visualizing the relationships between these selected features and fire likelihood/area. Later in the project, I worked with Eliza on our map. Specifically, I worked on rendering the map to show all the counties in the US, and and helped prepare to join the counties on the map to the county-level temperature and precipitation data. I also worked on the map’s design. Finally, I tried to find additional weather data for the US (such as wind speed), but was largely unsuccessful.\nWright: I worked with Madeleine at the beginning of the project doing some data preparation. Specifically, I worked on formatting the Algeria data set so that it could be treated as binary fire/no fire data. I then worked on data vizualizations, making graphs of the relationships between different features and fire occurrence/scale. I wasn’t directly involved with the modeling, but after the models were trained I ran them on some test observations and vizualized their performance. Finally, I worked on a short research essay about the ethics of using Machine Learning for natural hazard prediction, prevention, and adaptation.\nEliza: I helped with initial downloading and importing of the Portugal and Algeria datasets and splitting them into train and test sets. I initially worked on training models on the Portugal dataset (with little success) and did some research to figure out how the authors of the paper that the dataset originated from achieved higher accuracies. I did some of the initial experimentation with log-transforming our y values, but then I pivoted to working on the mapping tool and Nhi took this part of the project over. I definitely spent most of my time and effort working on the mapping tool for this project with Madeleine. I did extensive research on US county-level meteorological data and identified and downloaded the nClimGrid dataset that we ended up using for our US predictions. I did a lot of the data cleaning of these datasets prior to generating our maps, which Madeleine took the lead on, and I helped streamline the map-making process after creating our first map by converting a lot of our code into reusable functions. For the blog post, I took lead on the Materials and Methods section with Nhi, specifically focusing on the mapping tool methods, and the Abstract.\nNhi: Towards the beginning of the project, I helped with cleaning up our datasets and making sure they are ready to be trained on. For the majority of the project, I did a lot of the training and testing for both of the Portuguese and Algerian datasets. The Algerian dataset was a lot easier to train on, and I was able to obtain a very good accuracy score early on. I trained the Algeria dataset on different models and with different combinations of features to see which would yield the highest accuracy. I did a lot of cross validation and fine tuning to make sure the models weren’t overfitting. Then I tested the highest scoring model on the Algerian testing dataset. Eliza started with training the Portuguese dataset, but did not get very far since this was a much harder dataset to train on. Thus, I took over the modeling for this as well. I started by training the dataset on other models since Eliza had only trained on two models to start. I wasn’t very successful with the other models either, so I explored different ways to transform the target labels since the majority of the labels were skewed towards 0’s. I did a lot of digging around on scikit-learn and while not ideal, I transformed the labels via lab encoder, into binary labels, and into different ranges of the target labels. I then trained different combinations of the features and transformed target labels on different models. I analyzed these models to see how they performed against the testing dataset. Overall, I dissected a lot of different variations of the features and target labels. Additionally, I also combed through our code and cleaned them up. I also added a lot of the descriptors before and after each code section to explain our processes. For the blog post, I wrote the Values Statement, the dataset and modeling in the Materials and Methods section, and added a bit to the Results."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "href": "posts/final proj/Final Blog Post.html#ethics-of-ml-as-a-tool-for-natural-hazard-prediction-prevention",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention",
    "text": "Ethics of ML as a tool for Natural Hazard Prediction & Prevention\nMachine Learning is emerging as an important predictive tool in natural disaster prevention. Numerous studies have used machine learning to classify risk of natural hazards, from landslides, floods, and erosion in Saudi Arabia (Youssef et al. 2023), to avalanche risk in mountainous regions (Choubin et al. 2019), to area of forest fires in the tropics (Li et al. 2023). In fact, the Portuguese fire data used in this project was originally the topic of a 2007 paper focusing on training models on readily available real-time meteorological data to predict the area of wildfires (Cortez and Morais 2007).\nSo, there is a growing library of academic literature and projects using Machine Learning models to predict either the occurrence or the scale of natural hazards. Our project builds on these works, using many similar techniques and models. As our high testing accuracy for the Algerian dataset shows, machine learning clearly can be a powerful tool in natural hazard prediction.\nBut this numerical, scientific, approach comes with risks. As Cortez and Morais note in their study of Monteshino National Park in Portugal, most fires are caused by humans, which is why features like day of the week are important. Natural hazards only become natural disasters when they have a negative impact on people’s lives. As Youseff et al. observe in their study of multi-hazard prediction in Saudi Arabia, multiple hazards often occur at the same time, and the impact of natural hazards disproportionately affects impoverished and underdeveloped countries (Youssef et al. 2023). A risk of our approach is that our models focus only on the fires, disregarding both human influence on the landscape that may lead to increased risk of hazards (e.g. overdrawing from local water sources, deforestation, etc.), as well as the impact fires may have on humans. Predicting whether or not a fire will occur, or how large it will be, is only useful if it is applied to help those at risk from fires.\nWagenaar et al.’s 2020 paper “Invited perspectives: How machine learning will change flood risk and impact assessment” touches on some of these ethical considerations (Wagenaar et al. 2020). One risk they bring up is that improved knowledge of flood risk from ML models might result in protection of only high-value land or property owned by the wealthy. That example is certainly transferable to our fire hazard project. They also raise the question of privacy, noting that some people might not want it widely known that their home is considered “at-risk” for flooding, or other hazards. Finally, there is the question of data. The causes of hazards in one place may not cause the same hazard in another, so it is important to understand which human and geographic features influence hazard risk at a local scale rather than trying to train a one-size-fits-all model.\nWith all that in mind, our project needs to come with an asterisk. We have trained models for forest fire occurrence and scale in two specific places. It is therefore unreasonable to expect that our model will perfectly transfer to other places in the world. If our project were to be used for any application beyond the academic, we would need to make sure that its impact – whether that be for risk management, insurance policies, or some other application – be equitable and nondiscriminatory. We are just scratching the surface of using Machine Learning for natural hazard prevention, and while our results show it to be a powerful tool, we must also stay vigilant to make sure that it is a force for good."
  },
  {
    "objectID": "posts/final proj/Final Blog Post.html#personal-reflection",
    "href": "posts/final proj/Final Blog Post.html#personal-reflection",
    "title": "Final Project Blog Post: Wildfire Prediction Tool",
    "section": "Personal Reflection",
    "text": "Personal Reflection"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A write up of my final project, which uses machine learning models trained on meteorological data from Algeria and Portugal and applies them to assess wildfire risk in the US.\n\n\n\n\n\n\nMay 18, 2023\n\n\nEliza Wieman (in collaboration with Wright Frost, Madeleine Gallop, and Nhi Dang)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA blog post that trains a logistic regression classifier to predict employment status and then performs an audit of the model to assess racial bias in the model. This post contains some basic exploration of the data, model training, a bias audit of the model, and a discussion of bias within the model and the ethics of employment prediction tools.\n\n\n\n\n\n\nApr 2, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA blog post implementing linear regression using both analytical methods and gradient descent, experimenting with LASSO regularization to combat overfitting, and using my linear regression model to analyze trends in a dataset examining bikeshare usage in Washington DC.\n\n\n\n\n\n\nMar 29, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA blog post implementing logistic regression with gradient descent. I experiment with stochastic gradient descent versus regular gradient descent, as well as testing different learning rates and batch sizes.\n\n\n\n\n\n\nMar 9, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA blog post implementing the perceptron algorithm and testing it on three datasets ranging in complexity and difficulty to classify.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "testing quarto"
  }
]