[
  {
    "objectID": "posts/logistic-regression/logistic-regression-blog.html#implementing-logistic-regression-with-gradient-descent",
    "href": "posts/logistic-regression/logistic-regression-blog.html#implementing-logistic-regression-with-gradient-descent",
    "title": "Logistic Regression Blog",
    "section": "Implementing Logistic Regression with Gradient Descent",
    "text": "Implementing Logistic Regression with Gradient Descent\nImplementing logistic loss with gradient descent does not differ hugely from implementing the perceptron algorithm, which I do in my last blog post. Both algorithms follow a similar formula of intializing random weights, and iteratively performing weight updates and calculating loss until the algorithm converges (or max iterations are performed). The main differences are the way that the weight update is performed and the function used to calculate loss. In this implementation, the weights are updated by calculating the gradient of the loss function with current weights, and then multiplying this by a specified learning rate and subtracting it from the current weights. Gradient descent converges when the gradient is zero, meaning a local minimum is found, and the loss stops updating. In this case, logistic loss was used. Logistic loss was chosen because it is a convex function, which means that we can treat local minima as global minima, making it easier to solve for the optimal weights. To train our logistic regression model, we follow a similar pattern of fitting the model to training data and calculating accuracy and loss as we train to see if the model is learning successfully.\n\nTesting the logistic regression model\nTo train and experiment on our model we must first create some data. Here we create data that is not quite linearly separable, but is relatively close.\n\nfrom logistic import LogisticRegression # your source code\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAfter creating and visualizing some data, we fit it and visualize the loss and accuracy overtime.\n\nLR = LogisticRegression()\nLR.fit(X, y, .5, 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"loss\")\nplt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWe see that as the loss decreases, the accuracy increases, which makes sense. If we run the model numerous times, we see that the amount of time it takes for the model to converge varies, which is largely a result of the initial random weight chosen.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3,3])\n\n(-3.0, 3.0)\n\n\n\n\n\n\nLR.score_history[-1]\n\n0.915\n\n\nIf we visualize the line drawn by our final weights, we see that it does a very good job of classifying our data. When we print the final accuracy we get 0.915, which seems reasonable for data that is not linearly separable. If we run these cells several times, we see the same line drawn each time, even if the evolution of the loss and accuracy looks different. This suggests that our logistic regression algorithm is always converging to the global minima, even if it doesn’t always take the same amount of time.\n\n\nChanging the learning rate\nWe can alter values of alpha, the learning rate, to make our algorithm converge more quickly. However, if we make our learning rate too large, our algorithm will never converge because it will jump over the minima each time it makes an update. To see this, lets set our learning rate to 40.\n\nLR = LogisticRegression()\nLR.fit(X, y, 40, 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"loss\")\n#plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWe can see that when we do this, the algorithm never converges, but instead the loss bounces around between approximately 0.48 and 0.4 loss between epochs 10 and 1000.\n\n\nLogistic regression with stochastic gradient descent\nNext we implement logistic regression with stochastic gradient descent. This function uses the same principal of gradient descent, but it splits the data into batches and performs a weight update for each batch before updating the loss and score. By performing multiple updates within each epoch, the algorithm will likely converge more quickly.\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 1000, 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"loss\")\n#plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(LR.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3,3])\n\n(-3.0, 3.0)\n\n\n\n\n\n\nLR.score_history[-1]\n\n0.915\n\n\nOur stochastic algorithm also draws the same line, although we need to use a smaller learning rate to allow the algorithm to converge, and we see small fluctuations at higher epochs.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  max_epochs = 1000, \n                  batch_size = 10, \n                  alpha = .05)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWhen we compare the stochastic versus regular gradient descent, we can see that the stochastic loss decreases more quickly at the beginning, but we see small fluctuations in loss at later timesteps.\nNow lets compare a series of batch sizes and see how it affects how quickly the algorithm converges.\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 100, 10)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size = 10\")\n#plt.plot(np.arange(num_steps) + 1, LR.score_history, label = \"accuracy\")\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 100, 20)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size = 20\")\n\nLR = LogisticRegression()\n\nLR.fit_stochastic(X, y, .01, 100, 50)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"batch size = 50\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\n\n\n\nWe can see that as we increase the batch size, the algorithm takes longer to converge. This makes sense, as the power of stochastic gradient descent lies in performing several updates within each epoch. As we increase the batch size, we decrease the number of updates performed in each epoch, requiring more epochs for the algorithm to converge."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#implementing-perceptron",
    "href": "posts/perceptron/perceptron_blog.html#implementing-perceptron",
    "title": "Perceptron Blog",
    "section": "Implementing Perceptron",
    "text": "Implementing Perceptron\nThe perceptron update was implemented using: \\[\\mathbf{\\tilde{w}}^{(t+1)}=\\mathbf{\\tilde{w}}^{(t)}+\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)y_i\\mathbf{\\tilde{x}}_i\\] (Equation 1 in https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-perceptron.html#source-code). This update works by calculating the dot product of the current weights and the features of the selected point and then multiplying it by the true label of that point. If the signs of the dot product and the label are different then the weights are updated, otherwise the weights remain the same. In the case that an update does occur, the new weights are calculated by multiplying the features of the selected point by the actual label and adding that to the current weights. Since we multiply the selected points features’ by the point’s true label, we are always moving our weights in the direction of that label, improving the perceptron’s ability to classify that specific point. By iterating through the points randomly and performing updates, the perceptron algorithm eventually converges if the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#testing-the-algorithm",
    "href": "posts/perceptron/perceptron_blog.html#testing-the-algorithm",
    "title": "Perceptron Blog",
    "section": "Testing the Algorithm",
    "text": "Testing the Algorithm\nThree experiments were performed to verify that the perceptron algorithm was implemented correctly.\n\nExperiment 1: Linearly Separable Data\nFirst the perceptron was fit on linearly separable data with two features.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n100 data points were generated, each with two features and a label of 1 or 0. The data was designed to be linearly separable (meaning the perceptron should converge if run for long enough).\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe algorithm was fit to the data with a maximum of 1000 iterations.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nIf we print the weight vector, we see three values. The first value is the weight of feature 1, the second is the weight of feature 2, and the third is the bias.\n\np.w\n\narray([2.10557404, 3.1165449 , 0.25079936])\n\n\nWe can also plot the accuracy as the model is fit.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIf we print the accuracy, we see that it is 1.0, we classified all of our data correctly!\n\np.score(X, y)\n\n1.0\n\n\nWe see this too when we draw the line specified by our weights. The line perfectly separates the two types of data.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-4,4])\n\n(-4.0, 4.0)\n\n\n\n\n\n\n\nExperiment 2: Not Linearly Separable Data\nNext let’s see how our algorithm runs on data that isn’t linearly separable. First we’ll define some new data. We still have 100 points, each with two features and a label, but when we visualize the data below we can see that it is not clustered as nicely.\n\nnp.random.seed(123)\n\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nA new perceptron was fit and the weights were printed. Like with Experiment 1, we see a weight vector with three values.\n\np2 = Perceptron()\np2.fit(X, y, max_steps = 1000)\np2.w\n\narray([ 3.25206473,  1.64360407, -1.10792451])\n\n\nWhen we plot and print the accuracy over time, we see that we were able to achieve an accuracy of 0.89 after 1000 runs. This makes sense, as an accuracy of 1.0 is required for the algorithm to end early, which is impossible on this dataset.\n\nfig = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X,y)\n\n0.89\n\n\nWhen we visualize the line that the perceptron specified, we see that it’s pretty good. It seems unlikely that the accuracy could get much better on this dataset.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nplt.ylim([-3.5,3.5])\n\n(-3.5, 3.5)\n\n\n\n\n\n\n\nExperiment 3: Data with More than 2 Features\nFor the last experiment we see if the perceptron can classify data with more than two features. I generated a dataset of 100 points, still with binary labels, but this time each data point has five features.\n\nX, y = make_blobs(n_samples = 100, n_features = 5, centers = 2)\n\nNow when we fit our perceptron and print the weights we get a six element vector, where the first five values correspond to the features and the final value corresponds to the bias. Our perceptron generalizes to data with more than two features!\n\np3 = Perceptron()\np3.fit(X,y,max_steps = 1000)\np3.w\n\narray([ 2.02291666,  6.08425872,  0.74441411,  5.17387787, 10.87406143,\n       -1.69055338])\n\n\nWhen we plot and print the accuracy of our perceptron we see that we achieve a loss of 1.0. This suggests that the dataset I generated is linearly separable.\n\nfig = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X,y)\n\n1.0"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#runtime-of-the-perceptron-update",
    "href": "posts/perceptron/perceptron_blog.html#runtime-of-the-perceptron-update",
    "title": "Perceptron Blog",
    "section": "Runtime of the Perceptron Update",
    "text": "Runtime of the Perceptron Update\nEach perceptron update (see Equation 1) is performed on only one data point, so the runtime of a single update is independent of the number of data points in the provided dataset. Performing the dot product \\(\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle\\) requires p+1 multiplications and p additions, for a runtime of O(p) (where p = number of features). The remaining operations within the expression \\(\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)\\) are scalar multiplication or boolean comparison, so the total runtime of this expression is O(p), dominated by the dot product. This expression produces a scalar which is multiplied by \\(y_i\\), another scalar, and \\(\\mathbf{\\tilde{x}}_i\\), which has dimensions (p+1)x1. Multiplying two scalars by a vector with dimensions (p+1)x1 takes O(p) time, so the runtime of \\(\\mathbb{1}(y_i\\langle\\mathbf{\\tilde{w}}^{(t)}\\mathbf{,\\tilde{x}}_i\\rangle<0)y_i\\mathbf{\\tilde{x}}_i\\) is O(p+p). The result of this expression is then added to the prior weight vector, which requires p+1 additions. Thus the total runtime of the update is O(p+p+p) = O(3p), which simplifies to O(p), a linear runtime determined by the number of features."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins.html",
    "href": "posts/penguins.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "train.head()\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post implementing logistic regression with gradient descent. I experiment with stochastic gradient descent versus regular gradient descent, as well as testing different learning rates and batch sizes.\n\n\n\n\n\n\nMar 9, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA blog post implementing the perceptron algorithm and testing it on three datasets ranging in complexity and difficulty to classify.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nEliza Wieman\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "testing quarto"
  }
]